# SPDX-FileCopyrightText: Copyright (c) 2023 - 2025 NVIDIA CORPORATION & AFFILIATES.
# SPDX-FileCopyrightText: All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.



output_dir: "runs"
checkpoint_dir: null  # Optional: set custom checkpoint path, defaults to output_dir
run_id: "surface/bfloat16"

# Performance considerations:
precision: bfloat16 # float32, float16, bfloat16, or float8
compile: true
profile: false

# Training configuration
training:
  num_epochs: 501 # Add one to save at 250
  save_interval: 25  # Save checkpoint every N epochs

  # StepLR scheduler: Decays the learning rate by gamma every step_size epochs
  scheduler:
    name: "StepLR"
    params:
      step_size: 100    # Decay every 200 epochs (set X as desired)
      gamma: 0.5        # Decay factor

  # Optimizer configuration
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-3
    weight_decay: 1.0e-4
    betas: [0.9, 0.999]
    eps: 1.0e-8

# Model configuration
model:
  _target_: physicsnemo.models.transolver.Transolver
  functional_dim: 2 # Input feature dimension
  out_dim: 4        # Output feature dimension
  embedding_dim: 6  # Spatial embedding dimension
  n_layers: 8       # Number of transformer layers
  n_hidden: 256     # Hidden dimension
  dropout: 0.0      # Dropout rate
  n_head: 8         # Number of attention heads
  act: "gelu"       # Activation function
  mlp_ratio: 2      # MLP ratio in attention blocks
  slice_num: 512     # Number of slices in physics attention
  unified_pos: false # Whether to use unified positional embeddings
  ref: 8            # Reference dimension for unified pos
  structured_shape: null
  use_te: false     # Use transformer engine
  time_input: false # Whether to use time embeddings
  plus: false


# Data configuration
data:
  train:
    data_path: /lustre/fsw/portfolios/coreai/projects/coreai_modulus_cae/datasets/drivaer_aws/domino/train/
  val:
    data_path: /lustre/fsw/portfolios/coreai/projects/coreai_modulus_cae/datasets/drivaer_aws/domino/val/
  max_workers: 8
  normalization_dir: "src/"  # Directory for normalization files
  preload_depth: 1
  pin_memory: true
  resolution: 300_000
  mode: surface
  # Preprocessing switches:
  # (Changing thes will change the embedding dim)
  include_normals: true
  include_sdf: false
  translational_invariance: true
  scale_invariance: true
  reference_scale: [12.0, 4.5, 3.25]
  data_keys:
    - "surface_fields"
    - "surface_mesh_centers"
    - "surface_normals"
    - "surface_areas"
    - "air_density"
    - "stream_velocity"
    - "stl_faces"
    - "stl_centers"
    - "stl_coordinates"
  include_geometry: false
  broadcast_global_features: true
  return_mesh_features: false

# Logging configuration
logging:
  level: INFO
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'